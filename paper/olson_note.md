# AprilTag： 一个健壮而灵活的视觉基准系统

> 使用自然出现的特征一直是机器感知的重点，但人造特征在一些实验中也扮演了很重要的角色，可以简化一些并不关注自然特征的系统。论文设计了一种使用二维码的视觉基准系统，可以从单张图像中实现六个自由度上的定位。这一系统融合了之前的直线检测、数字编码技术，对遮挡、扭曲、镜头失真有更好的效果。虽然在概念上类似于ARTag系统，但这一方法是完全开源的，算法也有详细的文档。

## Introduction

虽然和QR码很像，但视觉基准码有完全不同的目标和应用。使用QR码时，通常需要将摄像头与标签对齐，然后以相当高的分辨率拍摄它，获得数百个字节，比如一个网址。相比之下，视觉基准码只承载很小的信息（也许是12位），但即使它的分辨率非常低、光线不均匀、旋转异常或隐藏在图像的角落里，也能被自动检测和定位。为了便于检测，视觉基准码包含更少的信息。

视觉基准系统最著名的应用可能是在增强现实中，这刺激了几个流行系统的发展，包括ARToolkit和ARTag。现实世界的物体可以用视觉基准系统定位，叠加上虚拟生成的图像。同样，视觉基准还可用于基本动作捕捉。

设计一个优秀的视觉基准系统的难点来自于两方面：视觉检测以及信息编码。

下文分三个部分介绍AprilTag：标签检测器、编码系统、评估对比。

## 检测器

检测器的主要功能就是在图片中寻找可能存在的标签，或者说“内部颜色深的方形”。

检测器设计为很低的假阴率，所以就有一个很高的假阳率。（真假表示识别正确与否，阴阳表示识别的结果是真或假）编码系统会补偿这一过高的假阳率。

### 直线检测

> 计算像素的梯度→按照梯度大小排序→以梯度方向作为约束进行聚类→最小二乘计算直线

我们的方法从检测图像中的线开始。我们的方法与ARTag检测器的基本方法相似，计算每个像素点的梯度方向和梯度大小，并将像素点聚类成梯度方向和梯度大小相似的分量。

该聚类算法类似于Felzenszwalb的基于图的方法：创建一个图，每个节点代表一个像素。在相邻像素之间添加边，边权等于像素在梯度方向上的差值。然后将这些边按边权递增的方式进行排序处理：对于每条边，我们测试其两侧像素所属的集群是否应该连接在一起。假设一个集群n，我们把其中像素梯度方向的范围记为D(n)，梯度大小的范围记为M(n)。也就是说，D(n)和M(n)是标量值，分别表示梯度方向和梯度大小的最大值和最小值之差。对于D()，必须小心处理2π周期。然而，由于有用的边的跨度要比π小得多，这就很简单了。

给定n和m两个集群，如果满足以下两个条件，我们将它们连接在一起：

$$D(n \cup m) \leq \min (D(n), D(m))+K_{D} /|n \cup m|$$

$$M(n \cup m) \leq \min (M(n), M(m))+K_{M} /|n \cup m|$$

条件由[19]改编而来，可以直观理解：D()和M()值小，表示分量内部变化小。如果两个集群的联合和单独使用的集群一样一致，则将它们连接在一起。K<sub>D</sub>、K<sub>M</sub>参数允许分量内变化的适度增加，但随着分量变大，这种变化会迅速缩小。在早期的迭代中，K参数本质上允许每个集群“学习”其簇内的变化。在我们的实验中，我们使用了K<sub>D</sub>=100和K<sub>M</sub>=1200，尽管该算法在广泛的值范围内都工作得很好。

（？）由于性能原因，边缘权值被量化并存储为定点数。这允许边缘使用线性时间计数排序[20]。使用unionfind算法[20]可以有效地进行实际的合并操作，并将梯度方向和幅度的上界和下界存储在一个简单的数组中，该数组由每个分量的代表成员索引。

这种基于梯度的聚类方法对图像中的噪声很敏感，即使是少量的噪声也会引起局部梯度方向的变化，抑制了成分的增长。解决这个问题的方法是对图像进行低通滤波。与其他问题域不同，这种过滤可以模糊图像中的有用信息，标签的边缘本质上是大规模的特征（特别是与数据字段相比），因此这种过滤不会导致信息丢失。我们推荐σ=0.8。

（？）聚类操作完成后，使用传统的最小二乘方法将线段附加到互相连接的组件上，加权每个点的梯度大小（见图3）。我们调整每个线段，使其阴暗面在左，亮面在右。在处理的下一个阶段中，这允许我们在每个四轴上执行一个缠绕规则。

分割算法是我们的检测方案中最慢的阶段。作为一种解决方案，这个分割可以用图像的一半分辨率执行，以提高速度。子采样操作可以与推荐的低通滤波器有效地结合。这种优化的结果是适度减少检测范围，因为非常小的矩形可能不再被检测。

### 矩形检测

> 深度优先搜索→按逆时针顺序考虑线段组合→二维查找表加速

此时，已经为图像计算了一组有向线段。下一个任务是找到一系列的线段，形成一个矩形，同时对于遮挡和噪声尽可能健壮。

我们的方法基于深度为4的递归深度优先搜索：搜索树的每一层都向矩形添加一条边。在深度1，我们考虑所有的线段。在深度2到4，我们考虑这些线段：它们的起点和上一个线段的终点足够“接近”，且遵循逆时针的旋转顺序。通过调整“接近”的阈值来实现对遮挡和分割错误的健壮性：通过使阈值较大，可以处理边缘周围的显著间隙。我们的“接近”阈值是两倍的线长加上五个额外的像素。这是一个很大的阈值，导致了低的假阴性率，但也导致了高的假阳性率。

我们添加了一个二维查找表，以加速对起点在某点附近的线段的查询。通过这种优化，以及对不遵守缠绕规则或多次使用一个线段的前期筛选，矩形检测算法只占总计算需求的一小部分。

（？）一旦找到了四行，就会创建一个候选的矩形。这个矩形的角是组成它的线的交点。因为这些线条是用来自多个像素的数据拟合的，所以这些角的估计精确到一个像素的一小部分。

### 单应性变换和外参估计

（？）标签坐标系：[0 0 1]<sup>T</sup>在标签的中心，标签在x和y方向上扩展一个维度

（？）我们计算一个3×3的单应性矩阵，which能将二维点从标签坐标系中以齐次坐标投影到二维图像坐标系。这一步是利用直接线性变换(DLT)算法[22]计算。注意，由于单应性在齐次坐标中投影点，因此只按比例定义。

（？）标签的位置和方向的计算需要额外的信息:相机的焦距和标签的物理尺寸。由DLT计算的3×3单应性矩阵可以写成3×4摄像机投影矩阵P(假设已知)和4x3截断的外参矩阵E的乘积。外参矩阵通常为4×4，但在标签坐标系中，标签上的每个位置都在z=0处。因此，我们可以将每个标签坐标重写为二维齐次点，z轴忽略为0，并去除外参矩阵的第三列，形成截断的外参矩阵，我们将P的旋转分量表示为R<sub>ij</sub>，平移分量表示为T<sub>h</sub>，同时将未知的尺度因子表示为s：

$$\left[\begin{array}{cccc}
h_{00} & h_{01} & h_{02} \\
h_{10} & h_{11} & h_{12} \\
h_{20} & h_{21} & h_{22}
\end{array}\right]=s P E
=s\left[\begin{array}{cccc}
f_{x} & 0 & 0 & 0 \\
0 & f_{y} & 0 & 0 \\
0 & 0 & 1 & 0
\end{array}\right]\left[\begin{array}{ccc}
R_{00} & R_{01} & T_{x} \\
R_{10} & R_{11} & T_{y} \\
R_{20} & R_{21} & T_{z} \\
0 & 0 & 1
\end{array}\right]$$

（？）注意，我们不能直接解出E，因为P不是满秩的。我们可以将等式的右侧展开，将每个h<sub>ij</sub>的表达式写成联立方程组：

$$\begin{aligned}
h_{00} &=s R_{00} f_{x} \\
h_{01} &=s R_{01} f_{x} \\
h_{02} &=s T_{x} f_{x} \\
...
\end{aligned}$$

（？）这些都是很容易解决的，除了未知的元素比例因子s。然而,由于一个旋转矩阵的列必须所有的单元大小,我们可以限制的s大小。我们有两个旋转矩阵的列,所以我们计算年代的几何几何平均大小。通过要求标签出现在摄像机前面，即T < 0，可以恢复s的符号。通过计算已知两列向量的叉积，可以求出旋转矩阵的第三列。因为一个旋转矩阵的列必须是标准正交的。

（？）上述DLT过程和归一化过程并不能保证旋转矩阵是严格标准正交的。为了修正这个，我们计算极坐标decom-。在使误差[23]的弗洛贝尼厄斯矩阵范数最小化的同时，得到一个合适的旋转矩阵。

## 有效负载解码

> 黑白阈值

最后一个任务是从有效负载字段读取比特位。我们通过计算每个比特在标签坐标上的位置，使用单应性变换将其转换到图像坐标下，然后对结果像素进行阈值处理来实现。为了增强对光照的健壮性，我们使用了一个空间变化的阈值。(不仅仅是不同标签之间的光照变化，同一个标签的不同位置光照也可能不同)

具体来说，我们建立了“黑色”像素强度的空间变化模型，以及另一个“白色”像素强度的模型。我们使用标签的边界（其中包含已知的白色和黑色像素），来学习这个模型(见图4)。我们使用以下强度模型：

$$ I(x, y)=A x+B x y+C y+D $$

该模型有4个参数，用最小二乘回归很容易计算。我们建立了两个这样的模型，分别给黑白两色。解码数据时使用的阈值就是黑白模型预测强度值的平均值。

## 编码系统

一旦数据从正方形中解码出来，就要靠编码系统来检查其有效性。编码系统的目标是:最大化可识别码的数量；最大化可检测到或纠正的比特位错误；最小化假阳性/标签间混淆率；最小化每个标签的总比特数(因此缩小标签的大小)。

这些目标常常是相互冲突的，因此给定的代码代表了一种权衡。在本节中，我们将描述一种新的基于lexicodes的编码系统，它比以前的方法具有显著的优势。我们的过程可以生成具有各种属性的lexicodes，允许用户使用最适合自己需要的代码。

### 方法

> 命名规则→考虑四个旋转方向→具有混淆性的码

我们建议使用一个修改的lexicodes[24]。经典的lexicodes是由两个常量参数化的：**每个码字的比特数n和任意两个码字之间的最小汉明距离d**。lexicodes可以校正(d-1)/2位错误，检测d/2位错误。为了方便起见，我们将**最小汉明距离为10的36比特编码表示为36h10码**。

lexicode的名称来源于用于生成有效码字的启发式：候选码字按照字典顺序(从最小到最大)考虑，当它们与之前添加到码本的每个码字至少有d的距离时，将新码字添加到码本中。虽然很简单，这种方案往往是非常接近最优的。

在视觉基准系统的应用情况下，编码方案必须对旋转具有健壮性。换句话说，当标签旋转90度、180度或270度时，它与其他代码的汉明距离仍然是d。标准的lexicode生成算法不能保证此属性。然而，标准生成算法可以简单地扩展来支持这一点：在测试一个新的候选码字时，简单地确保所有四次旋转都有所需的最小汉明距离。lexicode算法可以很容易地添加约束，这是我们方法的一个优点。

一些码字，尽管满足汉明距离限制，却是糟糕的选择。例如，由所有零组成的码字将生成一个看起来像单个黑色方块的标记。这种简单的几何图形经常出现在自然场景中，导致假阳误报。例如，ARTag编码系统明确禁止两种编码，因为它们很可能偶然产生。

我们没有手动识别有问题的码，而是通过拒绝简单几何模式的候选码来修改lexicode生成算法。我们的度量是基于生成标签的2D图案所需的矩形数。例如，一个全黑的码只需要一个矩形，而黑-白-黑条纹需要两个矩形(一个大的黑色矩形加上第二个较小的白色矩形)。我们的假设是，复杂度高的标签模式(需要重构许多矩形)在自然界中出现的频率较低，从而导致假阳性率较低，本文后面的实验结果也支持了我们的假设。

利用这个思想，我们再次修改lexicode生成算法，以拒绝过于简单的候选码。我们使用一种简单的贪婪方法来近似估计标签码所需的矩形数量，这种方法反复考虑所有可能的矩形，并添加能够最大程度减少误差的矩形。由于标记通常非常小，所以这种计算不是瓶颈。复杂度小于阈值(在我们的实验中通常为10)的标签将被拒绝。结果部分显示了这种探索的适当性和有效性。

（？）最后，根据经验，我们发现了另一个能够降低假阳结果的改动。而不是按顺序测试码字（0，1，2，3...）， 我们考虑 （b，b+1p，b+2p，b+3p, ...）b是一个任意数，p是一个大质数，每一步都保留最小的n位。直观上，该方法生成的标签在每位上的熵都较大；另一方面，字典顺序有利于小值编码。这种方法的缺点是创建的可区分的代码较少:字典顺序倾向于非常密集地打包码字，而更随机的顺序导致码字的效率较低。

总结一下，我们使用的lexicode系统可以为任意大小的标签(例如，3x3, 4x4, 5x5, 6x6)和最小汉明距离生成码。我们的方法明确地保证了每个标签的所有四个旋转的最小汉明距离，并去除了几何复杂度低的标签。计算标签可能是一个复杂度较高的操作，但是可以离线完成。小的标签(5x5)可以在秒或分钟内计算，但较大的标签(6x6)可能需要几天的CPU时间。我们的软件已经计算并发布了许多有用的码族，大多数用户不需要生成自己的码系列。

### 误差校正的分析

> 误码率与最小汉明距离

（？）理论上的假阳率很容易估计出来。假设一个假的矩形被识别，并且其中的比特是随机的。假阳性的概率是合法的码字与所有可能的码字总数2"的比率。更激进的纠错会增加这个比率，因为它会增加合法码字的数量。纠错位带来的假阳率增加如下表所示。

当然，36h15编码的更好性能是有代价的：它只有27个可区分的码字，而36h10有2221个可区分的码字。

我们的编码方案比以前的方案(包括ARTag使用的方案和ARToolkitPlus使用的两个系统)强大得多:我们的编码系统实现码字之间更大的汉明距离，同时编码了大量可识别的id。最小汉明距离的改进如图5和下表所示:

为了解码一个可能损坏的码字，计算码字与码本中每个有效码字之间的汉明距离。如果最佳匹配的汉明距离小于用户指定的阈值，则报告检测。通过指定此阈值，用户能够控制假阳性和假阴性之间的权衡。

（这里指的损坏好像类似于比特传输时候的误码，在图像识别的场景中，可能受到更多的是光照干扰？模糊？畸变？污点？两种场景一样吗？）

该方法的一个缺点是，由于必须考虑每个有效码字，所以译码过程花费的时间与码本大小是线性的。但是，该系数非常小，与其他图像处理步骤相比，计算复杂度可以忽略不计。

（？）对于给定的编码方案，较大的标签(即36比特vs25比特)比较小的标签具有好得多的编码性能，尽管这是有代价的。在其他条件相同的情况下，一个给定的相机读取一个36位标签的最短有效距离将比一个16位或25位标签的短。然而，由于边界的4像素开销，较小的标签在距离上的好处是相当有限的；使用16位标签而不是36位标签只能预期提高25%的检测距离。因此，只有在对距离最敏感的应用中，更小的标签才有优势。

## 实验结果

### 基于经验的实验

我们想要回答的一个关键问题是，我们对假阳性率的分析预测是否适用于现实世界的图像。为了回答这个问题，我们使用了一个来自LabelMe数据集[26]的标准图像库，该数据集包含180,829张来自各种室内外的图像。因为这些图像中并不包含我们的标签，所以可以使用这些图像测量编码系统的假阳性率。

图6所示。经验误报与标签复杂性。我们的理论错误率假设在真实环境中，所有的码字都有相同的概率偶然出现。我们的假设是，现实环境偏向于具有低矩形覆盖复杂度的代码，而通过选择具有高矩形覆盖复杂度的码字，可以降低误报率。这个假设被上图所验证，上图显示了LabelMe数据集(实线)对于矩形覆盖复杂性c=2到c=10的经验假阳性率。在复杂性c=9和c=10时，假阳性率低于现实有效载荷随机分布的悲观模型所预测的率。

复杂度启发式的评估:我们首先希望评估我们的假设，即通过使用几何复杂度启发式来拒绝候选码字，可以降低假阳性率。为此，我们生成了10个25h9家族的变种，复杂度从1到10不等。在图6中，对于每一个复杂情况，假阳性率是被修正的最大比特错误数的函数。同时也显示了基于数据有效载荷随机分布假设的理论假阳性率。